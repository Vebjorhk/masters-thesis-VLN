{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c77d9-862b-4ddf-b855-3c1d53276226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as DT\n",
    "#from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "\n",
    "from utils import set_seed, load_dataset, get_device, load_system_prompt\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09d1ea-ea0b-42bc-a358-67b889b31e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bare endre p√• disse\n",
    "dataset = \"test\"\n",
    "inst = 2\n",
    "model = \"qwen2_5\"\n",
    "\n",
    "SAVE_DIR = f\"\"\n",
    "BASE_URL = \"\"\n",
    "\n",
    "API_KEY = \"\"\n",
    "headers = {\"api-key\" : API_KEY}\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.custom_model = \"\"\n",
    "        self.model = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "        self.version = 2 if model == \"qwen2_5\" else 1\n",
    "        self.cache_dir = \"\"\n",
    "        self.test_data = f\"./notebooks/R2R_{dataset}.json\" # inneholder alle paths\n",
    "        self.system_prompt = \"../prompts/system_prompt_panoramic.json\"\n",
    "        self.instruction_index = inst\n",
    "        self.use_flash_attention = True\n",
    "        self.width = 320\n",
    "        self.height = 240\n",
    "        self.seed = 32\n",
    "\n",
    "def get_path_instructions(data):\n",
    "    path_dict = {}\n",
    "\n",
    "    for p in data:\n",
    "        if p[\"path_id\"] not in path_dict.keys():\n",
    "            path_dict[p[\"path_id\"]] = p[\"instructions\"]\n",
    "        \n",
    "    return path_dict\n",
    "\n",
    "\n",
    "def format_prompt_v5(images_path, path_id, route_instruction, step_id, distance_traveled, candidates, processor, system_prompt):\n",
    "    # should be in the order: panorama_history, current_panorama, candidates views from left to right\n",
    "    images = os.listdir(images_path)\n",
    "    panoramas = [os.path.join(images_path, img) for img in images if img.startswith(\"pano\")]\n",
    "    panoramas = sorted(panoramas, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[-2]))\n",
    "\n",
    "    # these are probably sorted by default, however you might need to check\n",
    "    candidate_images = [os.path.join(images_path, img) for img in images if img.startswith(\"pano\") == False]\n",
    "    candidate_images = sorted(candidate_images, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "    \n",
    "    current_panorama = panoramas.pop(-1)\n",
    "\n",
    "    # route instruction, current step, cumulative distance\n",
    "    content = [\n",
    "        {\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : f\"Route instruction: {route_instruction}\\nCurrent step: {step_id}\\nCumulative Distance Traveled: {distance_traveled} meters\\n\\nPanorama Images from Previous Steps:\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # panorama from previous steps\n",
    "    for i, img in enumerate(panoramas):\n",
    "        content.append({\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : f\"\\n\\tPanorama at step: {i}: \"\n",
    "        })\n",
    "        content.append({\n",
    "            \"type\" : \"image\",\n",
    "            \"image\" : img\n",
    "        })\n",
    "\n",
    "    if len(panoramas) == 0:\n",
    "        content[0][\"text\"] += f\"[]\"\n",
    "\n",
    "    # current panorama\n",
    "    content.append({\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : f\"\\n\\nCurrent Panorama Image:\\n\\t\"\n",
    "    })\n",
    "\n",
    "    content.append({\n",
    "        \"type\" : \"image\",\n",
    "        \"image\" : current_panorama\n",
    "    })\n",
    "\n",
    "    # candidate directions\n",
    "    content.append({\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"\\n\\nCandidate Directions:\"\n",
    "    })\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        relative_angle = round(candidate[\"relative_angle\"], 0)\n",
    "        distance = round(candidate[\"distance\"], 2)\n",
    "        direction = \"Left\" if relative_angle < 0 else \"Right\"\n",
    "        \n",
    "        content.append({\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : f\"\\n\\tCandidate: {i}:\\n\\t\\tRelative angle: {abs(relative_angle)} degrees to the {direction}\\n\\t\\tDistance: {distance} meters\\n\\t\\tview: \"\n",
    "        })\n",
    "\n",
    "        content.append({\n",
    "            \"type\" : \"image\",\n",
    "            \"image\" : candidate_images[i]\n",
    "        })\n",
    "\n",
    "\n",
    "    # adds candidate STOP and the select cnadidate view \n",
    "    content.append({\n",
    "        \"type\" : \"text\",\n",
    "        \"text\" : \"\\n\\tCandidate: Stop\\n\\nNow, analyze the route instruction, your current position, and the available candidate directions. Select the candidate that best matches the instruction and helps you continue along the correct path. Answer on the format: Candidate: (and then the number)\"\n",
    "    })\n",
    "\n",
    "    messages = [\n",
    "        {\"role\" : \"system\", \"content\" : [{\"type\" : \"text\", \"text\" : system_prompt}]},\n",
    "        {\"role\" : \"user\", \"content\" : content},\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    panoramas.extend([current_panorama])\n",
    "\n",
    "    formatted_sample = {}\n",
    "    formatted_sample[\"text\"] = text\n",
    "    formatted_sample[\"candidates\"] = candidate_images\n",
    "    formatted_sample[\"panoramas\"] = panoramas\n",
    "\n",
    "    formatted_data = [formatted_sample] \n",
    "    formatted_data = DT.from_list(formatted_data)\n",
    "    return formatted_data\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.text = data[\"text\"]\n",
    "        self.panoramas = data[\"panoramas\"]\n",
    "        self.candidates = data[\"candidates\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.panoramas[index], self.candidates[index]\n",
    "\n",
    "# TODO: make the collatefunctor work with batches\n",
    "class CollateFunctor:\n",
    "    # No batch, therefore no max length\n",
    "    def __init__(self, processor, width, height):\n",
    "        self.processor = processor\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text, panoramas, candidates = batch[0]\n",
    "        label_start = processor.tokenizer(\"<|im_start|>assistant\\nCandidate: \", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        images = [Image.open(img) for img in panoramas]\n",
    "        candidate_images = [Image.open(img) for img in candidates]\n",
    "        #candidate_images = [Image.open(img).resize((self.width, self.height), Image.Resampling.LANCZOS) for img in candidates]\n",
    "        images.extend(candidate_images)\n",
    "        \n",
    "        processed = processor(text=text, images=[images], return_tensors=\"pt\")\n",
    "\n",
    "        prompt_input_ids = processed[\"input_ids\"]\n",
    "        input_ids = torch.cat([prompt_input_ids, label_start], dim=1)\n",
    "        \n",
    "        attention_mask = torch.ones(1, input_ids.shape[1])\n",
    "        processed[\"input_ids\"] = input_ids\n",
    "        processed[\"attention_mask\"] = attention_mask\n",
    "        \n",
    "        return processed\n",
    "\n",
    "# to save space we delete candidate images after use\n",
    "def delete_candidate_images(folder):\n",
    "    images = os.listdir(folder)\n",
    "    deletable = [img for img in images if img.startswith(\"pano\") == False]\n",
    "\n",
    "    for img in deletable:\n",
    "        file_path = os.path.join(folder, img)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "    \n",
    "\n",
    "# Networking functions\n",
    "def save_image_from_base64(base64_str, filename):\n",
    "    \"\"\"Decodes the base64 string and saves the image to a file.\"\"\"\n",
    "    # Decode the base64 string to bytes\n",
    "    image_bytes = base64.b64decode(base64_str)\n",
    "    \n",
    "    # Open the image from the decoded bytes and save it\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    filepath = os.path.join(SAVE_DIR, filename)\n",
    "    image.save(filepath)\n",
    "\n",
    "def handle_data(path_id, step_id, data):\n",
    "    # save panorama\n",
    "    save_image_from_base64(data['panorama'], f\"{path_id}/pano_step_{step_id}.png\")\n",
    "\n",
    "    # save candidates:\n",
    "    candidates = []\n",
    "    for i, candidate in enumerate(data[\"candidates\"].values()):\n",
    "        save_image_from_base64(candidate[\"image\"], f\"{path_id}/step_{step_id}_candidate_{i}.png\")\n",
    "        candidates.append({\n",
    "            \"relative_angle\" : candidate[\"relative_angle\"],\n",
    "            \"distance\" : candidate[\"distance\"]\n",
    "        })\n",
    "\n",
    "    return candidates\n",
    "    \n",
    "def start_episode(path_id):\n",
    "    \"\"\"Starts a new episode and returns the session ID.\"\"\"\n",
    "    url = f\"{BASE_URL}/episode/start_episode/{path_id}\"\n",
    "    response = requests.post(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Episode started for path_id: {path_id}\")\n",
    "        data = response.json()  # The response is now in JSON format\n",
    "        candidates = handle_data(path_id, 0, data)\n",
    "        return path_id, candidates\n",
    "    else:\n",
    "        print(f\"Error starting episode: {response.json()}\")\n",
    "        return None\n",
    "\n",
    "def end_episode(path_id):\n",
    "    \"\"\"Ends the current episode and returns some data\"\"\"\n",
    "    url = f\"{BASE_URL}/episode/end_episode/{path_id}\"\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Episode ended for path_id: {path_id}\\n\")\n",
    "        data = response.json()  # The response is now in JSON format\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error ending episode: {response.json()}\")\n",
    "        return None\n",
    "\n",
    "def take_action(session_id, action, step_id):\n",
    "    \"\"\"Takes an action in the specified session and saves the returned image.\"\"\"\n",
    "    url = f\"{BASE_URL}/take_action/{session_id}\"\n",
    "    params = {\"action\": action}\n",
    "    response = requests.post(url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        #print(f\"Action '{action}' taken in session '{session_id}'\")\n",
    "        data = response.json()  # The response is now in JSON format\n",
    "        candidates = handle_data(session_id, step_id, data)\n",
    "        return candidates\n",
    "    else:\n",
    "        print(f\"Error taking action: {response.json()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe5588-673c-4440-a5fe-f526f5aa20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "set_seed(args.seed)\n",
    "# kan v√¶re at den max pixel greia var problemet og at den ikke klarte √• see hele panorama bildet\n",
    "processor = AutoProcessor.from_pretrained(args.model, cache_dir=args.cache_dir)\n",
    "\n",
    "if args.version == 1:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                args.custom_model, \n",
    "                cache_dir=args.cache_dir,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                device_map=device\n",
    "            )\n",
    "\n",
    "else:\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                args.custom_model, \n",
    "                torch_dtype=torch.bfloat16,\n",
    "                cache_dir=args.cache_dir,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                device_map=device\n",
    "            )\n",
    "\n",
    "test_data = load_dataset(args.test_data)\n",
    "system_prompt = load_system_prompt(args.system_prompt)\n",
    "\n",
    "model.eval()\n",
    "print(\"model loaded to memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911f7e1-6500-4bcc-b36b-fd9a92692ffa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "collate_fn = CollateFunctor(processor, args.width, args.height)\n",
    "\n",
    "model_actions = {}\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 479\n",
    "    for i, path in enumerate(test_data[1320:]):\n",
    "        print(f\"Number: {i}\")\n",
    "        step_id = 0\n",
    "        path_id = path[\"path_id\"]\n",
    "        route_instruction = path[\"instructions\"][args.instruction_index]\n",
    "        cumulative_distance = 0\n",
    "\n",
    "        # to view the predictions\n",
    "        model_predictions = []\n",
    "\n",
    "        images_path = os.path.join(SAVE_DIR, f\"{path_id}\")\n",
    "        os.makedirs(images_path, exist_ok=True)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        model_prediction = None\n",
    "        session_id, candidates = start_episode(path_id)\n",
    "\n",
    "        # assumes the episode is ended if candidates == None\n",
    "        while step_id < 32 and model_prediction != \"Stop\" and candidates != None:\n",
    "            #prompt = format_prompt_v4(path_id, step_id, instruction, previous_actions, move_possible, system_prompt, processor, data_type=\"val\")\n",
    "            prompt = format_prompt_v5(images_path, path_id, route_instruction, step_id, cumulative_distance, candidates, processor, system_prompt)\n",
    "            \n",
    "            dataset = CustomDataset(prompt)\n",
    "            data_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "    \n",
    "            for batch in data_loader:\n",
    "                batch.to(device)\n",
    "                \n",
    "                outputs = model(**batch)\n",
    "                \n",
    "                argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "                model_prediction = processor.decode(argmax[-1]) # is -1 because it does not predict one more\n",
    "                model_predictions.append(model_prediction)\n",
    "\n",
    "                #print(f\"Model prediction: {model_prediction}\")\n",
    "                if model_prediction.isnumeric() and int(model_prediction) < len(candidates):\n",
    "                    selected_candidate = candidates[int(model_prediction)]\n",
    "                    cumulative_distance = round(cumulative_distance + selected_candidate[\"distance\"], 2)\n",
    "\n",
    "                delete_candidate_images(images_path)\n",
    "                \n",
    "                step_id += 1\n",
    "                candidates = take_action(path_id, model_prediction, step_id)\n",
    "                \n",
    "        delete_candidate_images(images_path)\n",
    "        model_actions[path_id] = model_predictions\n",
    "        end_episode(path_id)\n",
    "\n",
    "with open(f\"{SAVE_DIR}-actions.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(model_actions, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132f7ad-a2c2-4de6-93c8-5c36c536ea3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (masters-env)",
   "language": "python",
   "name": "masters-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
