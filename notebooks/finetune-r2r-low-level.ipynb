{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604911a-76a6-4275-b0f6-5e61b89f3b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "#from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration,  AutoProcessor\n",
    "from datasets import Dataset as DF\n",
    "\n",
    "from src import set_seed, load_dataset, get_device, load_system_prompt, change_labels, grouped_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff0075-9cc7-4173-859c-a81059d75749",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "def create_model(model_id, cache_dir, use_flash_attention):\n",
    "    if use_flash_attention:\n",
    "        return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_id, \n",
    "            cache_dir = cache_dir,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            device_map=device\n",
    "        )\n",
    "    \n",
    "    return Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        model_id, \n",
    "        cache_dir = cache_dir,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.text = data[\"text\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "        self.images = data[\"images\"]\n",
    "        self.path_id = data[\"path_id\"]\n",
    "        self.step_id = data[\"step_id\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.images[index], self.labels[index], self.path_id[index], self.step_id[index]\n",
    "\n",
    "# TODO: make the collatefunctor work with batches\n",
    "class CollateFunctor:\n",
    "    # No batch, therefore no max length\n",
    "    def __init__(self, processor, width, height):\n",
    "        self.processor = processor\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text, images, labels, path_id, step_id = batch[0]\n",
    "        action_tokens = processor.tokenizer(labels, return_tensors=\"pt\").input_ids\n",
    "        label_start = processor.tokenizer(\"<|im_start|>assistant\\nAction: \", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        #images = [Image.open(img).resize((self.width, self.height), Image.Resampling.LANCZOS) for img in images]\n",
    "        images = [Image.open(img) for img in images]\n",
    "\n",
    "        processed = processor(text=text, images=[images], return_tensors=\"pt\")\n",
    "\n",
    "        prompt_input_ids = processed[\"input_ids\"]\n",
    "        prompt_input_ids = torch.cat([prompt_input_ids, label_start], dim=1)\n",
    "        input_ids = torch.cat([prompt_input_ids, action_tokens], dim=1)\n",
    "        \n",
    "        labels = torch.cat(\n",
    "            [\n",
    "                # se på denne senere, altså om vi skal virkelig fokusere på alt det der andre chat greiene\n",
    "                torch.tensor([-100]*len(prompt_input_ids[0])).unsqueeze(0),\n",
    "                action_tokens\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "\n",
    "        attention_mask = torch.ones(1, input_ids.shape[1])\n",
    "        processed[\"labels\"] = labels\n",
    "        processed[\"input_ids\"] = input_ids\n",
    "        processed[\"attention_mask\"] = attention_mask\n",
    "        processed[\"gold_action\"] = action_tokens\n",
    "        processed[\"path_id\"] = path_id\n",
    "        processed[\"step_id\"] = step_id\n",
    "        \n",
    "        return processed\n",
    "\n",
    "\n",
    "def format_prompts_v3_5(dataset, processor, system_prompt, instruction_index, path, data_type=\"train\"):\n",
    "    root_path = os.path.join(path, data_type)\n",
    "    formatted_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        content = [\n",
    "            {\n",
    "                \"type\" : \"text\", \n",
    "                #\"text\" : f\"Route instruction: {sample['instructions'][instruction_index]}\\nPrevious images: \"\n",
    "                \"text\" : f\"Route Instruction: {sample['instructions'][instruction_index]}\\nCurrent Step: {sample['step_id']}\\nCummulative Distance Traveled: {sample['distance_traveled']}\\nImages from Previous Steps: \" \n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        images = sample[\"past_images\"]\n",
    "        images = [os.path.join(root_path, i) for i in sample[\"past_images\"]]\n",
    "\n",
    "        # HUSK DENNE HVIS DU SKAL PRØVE NOE ANNET\n",
    "        #if len(images) > 5:\n",
    "        #    images = images[-6:]\n",
    "        #    for img in images:\n",
    "        #        content.append({\"type\" : \"image\", \"image\" : img})\n",
    "\n",
    "        #else:\n",
    "        for img in images:\n",
    "            content.append({\"type\" : \"image\", \"image\" : img}) \n",
    "\n",
    "        if len(images) == 0:\n",
    "            content[0][\"text\"] += f\"[]\"\n",
    "\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\" : \"text\", \n",
    "                \"text\" : f\"\\nActions performed at Previous Steps: {sample['previous_actions'].__str__()}\\nCurrent image:\"\n",
    "            }\n",
    "        )\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\" : \"image\", \n",
    "                \"image\" : os.path.join(root_path, sample[\"current_image\"])\n",
    "            }\n",
    "        )\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\" : \"text\", \n",
    "                \"text\" : f\"\\nPossible actions: {sample['possible_actions'].__str__()}\\nNow predict the next action based on the input you have recived. Answer on the format: Action: (an the action you choose)\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\" : \"system\", \"content\" : [{\"type\" : \"text\", \"text\" : system_prompt}]},\n",
    "            {\"role\" : \"user\", \"content\" : content},\n",
    "        ]\n",
    "\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        #labels = f\"<|im_start|>assistant\\n{sample['gold_label']}<|im_end|>\\n<|endoftext|>\"\n",
    "        #labels = f\"<|im_start|>assistant\\n{sample['gold_label']}<|im_end|>\\n\"# might be some cause of the problem not adding the end of text\n",
    "        images.extend([os.path.join(root_path, sample[\"current_image\"])])\n",
    "        \n",
    "        formatted_sample = {}\n",
    "        formatted_sample[\"text\"] = text\n",
    "        formatted_sample[\"labels\"] = sample[\"gold_label\"]\n",
    "        formatted_sample[\"images\"] = images\n",
    "        #formatted_sample[\"gold_actions\"] = sample[\"gold_label\"]\n",
    "        # maybe remove for later\n",
    "        formatted_sample[\"path_id\"] = sample[\"path_id\"]\n",
    "        formatted_sample[\"step_id\"] = sample[\"step_id\"]\n",
    "\n",
    "        formatted_data.append(formatted_sample)\n",
    "\n",
    "    formatted_data = DF.from_list(formatted_data)\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "def run_epoch_train(model, processor, dataloader, optimizer, lr_scheduler, epoch, batch_size, grouped_data):\n",
    "    print(f\"Training model, epoch: {epoch}\")\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    batch_loss = 0\n",
    "    \n",
    "    model_loss_analyse = []\n",
    "\n",
    "    episode_counter = 0\n",
    "    batch_counter = 0\n",
    "    \n",
    "    l = 0\n",
    "    # this list contains the number of samples which to be devided with by when accumulating batches\n",
    "    batch_length_list = []\n",
    "    for i, (k, v) in enumerate(grouped_data.items()):\n",
    "        l += len(v)\n",
    "        \n",
    "        if (i % batch_size == 0 and i != 0) or i+1 == len(grouped_data):\n",
    "            batch_length_list.append(l)\n",
    "            l = 0\n",
    "\n",
    "    batch_length = batch_length_list[batch_counter]\n",
    "    print(batch_length)\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        gold_action = batch[\"gold_action\"]\n",
    "        path_id = batch[\"path_id\"]\n",
    "        step_id = batch[\"step_id\"]\n",
    "        \n",
    "        del batch[\"gold_action\"]\n",
    "        del batch[\"path_id\"]\n",
    "        del batch[\"step_id\"]\n",
    "        \n",
    "        batch.to(device)\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        batch_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        (loss/batch_length).backward()\n",
    "\n",
    "        wandb.log({\"step_loss\" : loss.item()})\n",
    "\n",
    "        argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "        model_prediction = processor.decode(argmax[-2])\n",
    "        gold = processor.decode(gold_action[0])\n",
    "\n",
    "        sample = {}\n",
    "        #sample[\"prompt\"] = processor.decode(batch[\"input_ids\"][0])\n",
    "        sample[\"loss\"] = loss.item()# dette var problemet\n",
    "        sample[\"path_id\"] = path_id\n",
    "        sample[\"step_id\"] = step_id\n",
    "        sample[\"gold_label\"] = gold\n",
    "        sample[\"argmax_result\"] = model_prediction\n",
    "\n",
    "        model_loss_analyse.append(sample)\n",
    "\n",
    "        if (episode_counter % batch_size == 0 and episode_counter != 0 and gold == \"Stop\") or (i + 1) == len(dataloader):\n",
    "            wandb.log({\"batch_loss\" : batch_loss/batch_length})\n",
    "            \n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step() # never remove this again, this is by the book and should alawys be like this\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients for next accumulation\n",
    "            \n",
    "            batch_loss = 0\n",
    "            batch_counter += 1\n",
    "            if batch_counter < len(batch_length_list):\n",
    "                batch_length = batch_length_list[batch_counter]\n",
    "\n",
    "        if gold == \"Stop\":\n",
    "            episode_counter += 1\n",
    "\n",
    "        if (i+1) % 500 == 0 or (i+1) == len(dataloader):\n",
    "            print(f\"step: {i}\")\n",
    "            print(f\"Path_id:  {path_id}, step_id: {step_id}\")\n",
    "            print(f\"step Loss: {loss.item()}\")\n",
    "            print(f\"Model Prediction: {repr(model_prediction)}\")\n",
    "            print(f\"Gold Action: {repr(gold)}\")\n",
    "            print()\n",
    "            softmax = F.softmax(outputs.logits, dim=2)\n",
    "            print(f\"probability for Stop on -2: {softmax[0, -2, 10674]}\")\n",
    "            print(f\"probability for Move on -2: {softmax[0, -2, 9860]}\")\n",
    "            print(f\"probability for Right on -2: {softmax[0, -2, 5979]}\")\n",
    "            print(f\"probability for Left on -2: {softmax[0, -2, 5415]}\")\n",
    "            print(f\"probability for <|im_end|> on -1: {softmax[0, -1, 151645]}\")\n",
    "            print(f\"probability for backslash n on -1: {softmax[0, -1, 198]}\")\n",
    "            print(f\"probability for <|endoftext|> on -1: {softmax[0, -1, 151643]}\")\n",
    "            print()\n",
    "            print(\"-\"*60)\n",
    "\n",
    "    \n",
    "    epoch_loss /= len(dataloader)\n",
    "    wandb.log({\"training_loss\": epoch_loss, \"learning_rate\": lr_scheduler.get_last_lr()[0]})\n",
    "\n",
    "    return epoch_loss, model_loss_analyse\n",
    "    \n",
    "\n",
    "@torch.no_grad\n",
    "def run_epoch_eval_or_test(model, processor, dataloader, mode=\"eval\"):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            gold_action = batch[\"gold_action\"]\n",
    "            path_id = batch[\"path_id\"]\n",
    "            step_id = batch[\"step_id\"]\n",
    "            \n",
    "            del batch[\"gold_action\"]\n",
    "            del batch[\"path_id\"]\n",
    "            del batch[\"step_id\"]\n",
    "        \n",
    "            batch.to(device)\n",
    "        \n",
    "            outputs = model(**batch)\n",
    "\n",
    "            total_loss += outputs.loss.item()\n",
    "\n",
    "            # greedy search for action\n",
    "            argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "            model_prediction = processor.decode(argmax[-2]).strip(\" \")\n",
    "            gold = processor.decode(gold_action[0])\n",
    "\n",
    "            predictions.append({\"gold\" : gold, \"model\" : model_prediction, \"path_id\" : path_id, \"step_id\" : step_id})\n",
    "            \n",
    "            if model_prediction == gold:\n",
    "                accuracy += 1\n",
    "\n",
    "\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: (x[\"path_id\"], x[\"step_id\"]), reverse=False)\n",
    "    path_dict = {}\n",
    "\n",
    "    for d in sorted_predictions:\n",
    "        if path_dict.get(d[\"path_id\"], None) != None:\n",
    "            path_dict[d[\"path_id\"]][\"gold\"].append(d[\"gold\"])\n",
    "            path_dict[d[\"path_id\"]][\"model\"].append(d[\"model\"])\n",
    "    \n",
    "        else:\n",
    "            path_dict[d[\"path_id\"]] = {}\n",
    "            path_dict[d[\"path_id\"]][\"gold\"] = [d[\"gold\"]]\n",
    "            path_dict[d[\"path_id\"]][\"model\"] = [d[\"model\"]]\n",
    "\n",
    "    gold_action_count = {\n",
    "        \"Left\" : len([i[\"gold\"] for i in sorted_predictions if i[\"gold\"] == \"Left\"]),\n",
    "        \"Right\" : len([i[\"gold\"] for i in sorted_predictions if i[\"gold\"] == \"Right\"]),\n",
    "        \"Move\" : len([i[\"gold\"] for i in sorted_predictions if i[\"gold\"] == \"Move\"]),\n",
    "        \"Stop\" : len([i[\"gold\"] for i in sorted_predictions if i[\"gold\"] == \"Stop\"]),\n",
    "    }\n",
    "    \n",
    "    model_label_recall = {\n",
    "        \"Left\" : len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Left\" and i[\"gold\"] == \"Left\"])/gold_action_count[\"Left\"],\n",
    "        \"Right\" : len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Right\" and i[\"gold\"] == \"Right\"])/gold_action_count[\"Right\"],\n",
    "        \"Move\" : len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Move\" and i[\"gold\"] == \"Move\"])/gold_action_count[\"Move\"],\n",
    "        \"Stop\" : len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Stop\" and i[\"gold\"] == \"Stop\"])/gold_action_count[\"Stop\"],\n",
    "    }\n",
    "\n",
    "    # Precision calculation\n",
    "    model_action_count = {\n",
    "        \"Left\": len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Left\"]),\n",
    "        \"Right\": len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Right\"]),\n",
    "        \"Move\": len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Move\"]),\n",
    "        \"Stop\": len([i[\"model\"] for i in sorted_predictions if i[\"model\"] == \"Stop\"]),\n",
    "    }\n",
    "    \n",
    "    model_label_precision = {\n",
    "        \"Left\": len([i for i in sorted_predictions if i[\"model\"] == \"Left\" and i[\"gold\"] == \"Left\"]) / model_action_count[\"Left\"] if model_action_count[\"Left\"] > 0 else 0.0,\n",
    "        \"Right\": len([i for i in sorted_predictions if i[\"model\"] == \"Right\" and i[\"gold\"] == \"Right\"]) / model_action_count[\"Right\"] if model_action_count[\"Right\"] > 0 else 0.0,\n",
    "        \"Move\": len([i for i in sorted_predictions if i[\"model\"] == \"Move\" and i[\"gold\"] == \"Move\"]) / model_action_count[\"Move\"] if model_action_count[\"Move\"] > 0 else 0.0,\n",
    "        \"Stop\": len([i for i in sorted_predictions if i[\"model\"] == \"Stop\" and i[\"gold\"] == \"Stop\"]) / model_action_count[\"Stop\"] if model_action_count[\"Stop\"] > 0 else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "    sr = 0\n",
    "    for path, d in path_dict.items():\n",
    "        if d[\"gold\"] == d[\"model\"]:\n",
    "            sr+=1\n",
    "            \n",
    "    success_rate = sr/len(path_dict)\n",
    "    total_loss /= len(dataloader)\n",
    "    accuracy = accuracy / len(dataloader)\n",
    "\n",
    "    if mode == \"eval\":\n",
    "        wandb.log({\n",
    "            \"validation_loss\" : total_loss, \n",
    "            \"validation accuracy\" : accuracy, \n",
    "            \"validation sucess rate\" : success_rate,\n",
    "            \"validation: Left recall\" : model_label_recall[\"Left\"], \n",
    "            \"validation: Right recall\" : model_label_recall[\"Right\"], \n",
    "            \"validation: Move recall\" : model_label_recall[\"Move\"],\n",
    "            \"validation: Stop recall\" : model_label_recall[\"Stop\"],\n",
    "            \"validation: Left precision\" : model_label_precision[\"Left\"], \n",
    "            \"validation: Right precision\" : model_label_precision[\"Right\"], \n",
    "            \"validation: Move precision\" : model_label_precision[\"Move\"],\n",
    "            \"validation: Stop precision\" : model_label_precision[\"Stop\"]\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        wandb.log({\n",
    "            \"test_loss\" : total_loss, \n",
    "            \"test accuracy\" : accuracy, \n",
    "            \"test sucess rate\" : success_rate,\n",
    "            \"test: Left recall\" : model_label_recall[\"Left\"], \n",
    "            \"test: Right recall\" : model_label_recall[\"Right\"], \n",
    "            \"test: Move recall\" : model_label_recall[\"Move\"],\n",
    "            \"test: Stop recall\" : model_label_recall[\"Stop\"],\n",
    "            \"test: Left precision\" : model_label_precision[\"Left\"], \n",
    "            \"test: Right precision\" : model_label_precision[\"Right\"], \n",
    "            \"test: Move precision\" : model_label_precision[\"Move\"],\n",
    "            \"test: Stop precision\" : model_label_precision[\"Stop\"]\n",
    "            \n",
    "        })\n",
    "\n",
    "    # infere these model actions from the test set as it is not shuffled\n",
    "    return total_loss, path_dict\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "        self.cache_dir = \"\"\n",
    "\n",
    "        # they should all have the same name\n",
    "        self.wandb_name = \"\"\n",
    "        self.outputs_path = \"./experiment-outputs/\"\n",
    "        self.model_checkpoint_path = \"\"\n",
    "        \n",
    "        self.train_data = \"/dataset/train/train_data.json\"\n",
    "        self.val_data = \"/datasetval/val_data.json\"\n",
    "        self.test_data = \"/dataset/test/test_data.json\"\n",
    "        self.dataset_root = \"/dataset/\"\n",
    "        self.system_prompt = \"./prompts/system_prompt_low_level.json\"\n",
    "        \n",
    "        self.lr = 0.00001\n",
    "        self.weight_decay = 0.1\n",
    "        self.warmup_ratio = 0.1\n",
    "        self.shuffle = False\n",
    "        self.epochs = 1\n",
    "        self.batch_size = 1\n",
    "        \n",
    "        self.notes = \"\"\n",
    "        \n",
    "        self.use_flash_attention = True\n",
    "        self.freeze_vision = True\n",
    "        self.instruction_index_train = 0\n",
    "        self.instruction_index_val = 2\n",
    "        self.width = 320\n",
    "        self.height = 240\n",
    "        self.seed = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585dc8f1-1d9c-407d-9a7d-3b4d1472add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # for saving outputs\n",
    "    if not os.path.exists(args.outputs_path):\n",
    "        os.makedirs(args.outputs_path)\n",
    "    \n",
    "    model = create_model(args.model, args.cache_dir, args.use_flash_attention)\n",
    "    processor = AutoProcessor.from_pretrained(args.model, cache_dir=args.cache_dir, max_pixels=args.width*args.height)\n",
    "\n",
    "    if args.freeze_vision:\n",
    "        for name, param in model.visual.named_parameters():\n",
    "            param.requires_grad = False  # Freeze parameter\n",
    "\n",
    "    train_data = load_dataset(args.train_data)\n",
    "    train_data = change_labels(train_data)\n",
    "    grouped_data = grouped_paths(train_data)\n",
    "\n",
    "    val_data = load_dataset(args.val_data)\n",
    "    val_data = change_labels(val_data)\n",
    "\n",
    "    test_data = load_dataset(args.test_data)\n",
    "    test_data = change_labels(test_data)\n",
    "\n",
    "    system_prompt = load_system_prompt(args.system_prompt)\n",
    "\n",
    "    val = format_prompts_v3_5(val_data, processor, system_prompt, args.instruction_index_val, args.dataset_root, data_type=\"val\")\n",
    "    val_dataset = CustomDataset(val)\n",
    "    collate_functor = CollateFunctor(processor, args.width, args.height)\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1, # Enforce batch size of 1 as phi-3.5 vision does not support a larger batch size out of the box (qwen does but i'll look at that later)\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_functor,\n",
    "    )\n",
    "\n",
    "    # Ok, dette var kanskje problemet med den andre modellen du lærte, men bare kjør freezed også prøver du på nytt uten freezed\n",
    "    total_steps = (len(grouped_data)/args.batch_size)*args.epochs\n",
    "    warmup_steps = int(args.warmup_ratio * total_steps)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"masters-thesis\",\n",
    "        name=args.wandb_name,\n",
    "        notes=args.notes,\n",
    "        # notes=\"This is the fine tuning of Qwen2-VL-2B-Instruct on the room-to-room dataset\",\n",
    "        job_type=\"fine-tune\",\n",
    "    \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"model\" :  args.model,\n",
    "            \"dataset\": \"room-to-room\",\n",
    "            \"epochs\": args.epochs,\n",
    "            \"batch_size\" : args.batch_size,\n",
    "            \"learning_rate\": args.lr,\n",
    "            \"weight_decay\" : args.weight_decay,\n",
    "            \"warmup_ratio\" : args.warmup_ratio,\n",
    "            \"optimizer\" : \"AdamW\",\n",
    "            \"schedular\" : \"linear_schedule_with_warmup\",\n",
    "            \"architecture\": \"transformer\",\n",
    "            \"seed\" : args.seed,\n",
    "            \"action_space\" : [\"Move\", \"Left\", \"Right\", \"Stop\"],\n",
    "            \"model_checkpoint_path\" : args.model_checkpoint_path,\n",
    "            \"train_data_path\" : args.train_data,\n",
    "            \"prompt_path\" : args.system_prompt,\n",
    "            \"outputs_path\" : args.outputs_path,\n",
    "            \"samples\" : len(train_data),\n",
    "            \"instruction_index_train\" : \"all three instructions\", #args.instruction_index_train,\n",
    "            \"instruction_index_val\" : args.instruction_index_val\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(args.epochs):\n",
    "        instruction = i\n",
    "        \n",
    "        if i > 2:\n",
    "            instruction = i - 3\n",
    "\n",
    "        print(f\"instruction number: {instruction}\")\n",
    "        \n",
    "        formatted_data = format_prompts_v3_5(train_data, processor, system_prompt, instruction, args.dataset_root)\n",
    "        \n",
    "        dataset = CustomDataset(formatted_data)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=1, \n",
    "            shuffle=False,\n",
    "            collate_fn=collate_functor\n",
    "        )\n",
    "\n",
    "        # calculating loss on validation data\n",
    "        val_loss, path_dict = run_epoch_eval_or_test(model, processor, val_dataloader)\n",
    "\n",
    "        with open(os.path.join(args.outputs_path, f\"val_path_dict_epoch_{i}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(path_dict, file, indent=4) \n",
    "        \n",
    "        # training model\n",
    "        train_loss, model_analyse = run_epoch_train(\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            dataloader=dataloader,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=scheduler,\n",
    "            epoch=i,\n",
    "            batch_size=args.batch_size,\n",
    "            grouped_data=grouped_data\n",
    "        )\n",
    "\n",
    "        with open(os.path.join(args.outputs_path, f\"model_analyse_{i}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(model_analyse, file, indent=4)\n",
    "\n",
    "        model_save_path = os.path.join(args.model_checkpoint_path, f\"checkpoint_{i}\")\n",
    "        model.save_pretrained(model_save_path)\n",
    "        print(f\"model saved at: {model_save_path}, epoch {i} finished\")\n",
    "\n",
    "\n",
    "        with open(os.path.join(args.outputs_path, f\"val_path_dict_epoch_{i}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(path_dict, file, indent=4) \n",
    "\n",
    "\n",
    "    #val_loss, val_path_dict = run_epoch_eval_or_test(model, processor, val_dataloader, mode=\"eval\")\n",
    "    end = time.time()\n",
    "    print(f\"training time: {(end-start)/60} minutes\")\n",
    "\n",
    "     # run test\n",
    "    test = format_prompts_v3_5(test_data, processor, system_prompt, args.instruction_index_val, args.dataset_root, data_type=\"test\")\n",
    "    test_dataset = CustomDataset(test)\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1, \n",
    "        shuffle=False,\n",
    "        collate_fn=collate_functor,\n",
    "    )\n",
    "    test_loss, test_path_dict = run_epoch_eval_or_test(model, processor, test_dataloader, mode=\"test\")\n",
    "\n",
    "    #with open(os.path.join(args.outputs_path, \"val_path_dict.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "    #    json.dump(val_path_dict, file, indent=4)\n",
    "\n",
    "    with open(os.path.join(args.outputs_path,\"test_path_dict.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(test_path_dict, file, indent=4)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c477e6e-0391-4a32-86e3-adafbd4b34ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in5550",
   "language": "python",
   "name": "in5550"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
