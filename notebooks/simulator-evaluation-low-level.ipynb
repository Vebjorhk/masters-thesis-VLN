{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd27533-0c9d-4239-87e9-f72afb2cbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import Dataset as DT\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration\n",
    "\n",
    "\n",
    "from utils import set_seed, load_dataset, get_device, load_system_prompt\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec0bbd-3947-4e2d-a3ec-1af7c121a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bare endre på disse\n",
    "dataset = \"test\"\n",
    "inst = 2\n",
    "adjust = False\n",
    "\n",
    "SAVE_DIR = f\"\"\n",
    "BASE_URL = \"\"\n",
    "\n",
    "API_KEY = \"\"\n",
    "headers = {\"api-key\" : API_KEY}\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.custom_model = \"\n",
    "        self.model = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "        self.version = 2\n",
    "        self.cache_dir = \"\"\n",
    "        self.test_data = f\"./notebooks/R2R_{dataset}.json\" # inneholder alle paths\n",
    "        self.system_prompt = \"./prompts/system_prompt_v3_5_no_adjust.json\"\n",
    "        self.instruction_index = inst\n",
    "        self.use_flash_attention = True\n",
    "        self.width = 320\n",
    "        self.height = 240\n",
    "        self.seed = 32\n",
    "\n",
    "def get_path_instructions(data):\n",
    "    path_dict = {}\n",
    "\n",
    "    for p in data:\n",
    "        if p[\"path_id\"] not in path_dict.keys():\n",
    "            path_dict[p[\"path_id\"]] = p[\"instructions\"]\n",
    "        \n",
    "    return path_dict\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.text = data[\"text\"]\n",
    "        self.images = data[\"images\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.images[index]\n",
    "\n",
    "# TODO: make the collatefunctor work with batches\n",
    "class CollateFunctor:\n",
    "    # No batch, therefore no max length\n",
    "    def __init__(self, processor, width, height):\n",
    "        self.processor = processor\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text, images = batch[0]\n",
    "        label_start = processor.tokenizer(\"<|im_start|>assistant\\nAction: \", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        images = [Image.open(img) for img in images]\n",
    "\n",
    "        processed = processor(text=text, images=[images], return_tensors=\"pt\")\n",
    "\n",
    "        prompt_input_ids = processed[\"input_ids\"]\n",
    "        input_ids = torch.cat([prompt_input_ids, label_start], dim=1)\n",
    "\n",
    "        attention_mask = torch.ones(1, input_ids.shape[1])\n",
    "        processed[\"input_ids\"] = input_ids\n",
    "        processed[\"attention_mask\"] = attention_mask\n",
    "        \n",
    "        return processed\n",
    "\n",
    "\n",
    "def format_prompt_v3_5(images_path, step_id, route_instruction, distance_traveled, previous_actions, move_possible, processor, system_prompt):\n",
    "    images = os.listdir(images_path)\n",
    "    images = [os.path.join(images_path, img) for img in images]\n",
    "    images = sorted(images, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "    current_image = images.pop(-1)\n",
    "    \n",
    "    content = [\n",
    "            {\n",
    "                \"type\" : \"text\", \n",
    "                #\"text\" : f\"Route instruction: {sample['instructions'][instruction_index]}\\nPrevious images: \"\n",
    "                \"text\" : f\"Route Instruction: {route_instruction}\\nCurrent Step: {step_id}\\nCummulative Distance Traveled: {distance_traveled}\\nImages from Previous Steps: \" \n",
    "            },\n",
    "        ]\n",
    "\n",
    "    for img in images:\n",
    "        content.append({\"type\" : \"image\", \"image\" : img}) \n",
    "\n",
    "    if len(images) == 0:\n",
    "        content[0][\"text\"] += f\"[]\"\n",
    "\n",
    "    content.append(\n",
    "            {\n",
    "                \"type\" : \"text\", \n",
    "                \"text\" : f\"\\nActions performed at Previous Steps: {previous_actions.__str__()}\\nCurrent image:\"\n",
    "            }\n",
    "        )\n",
    "    content.append(\n",
    "            {\n",
    "                \"type\" : \"image\", \n",
    "                \"image\" : current_image\n",
    "            }\n",
    "        )\n",
    "    if move_possible:\n",
    "        possible_actions = [\"Left\", \"Right\", \"Move\", \"Stop\"]\n",
    "\n",
    "    else:\n",
    "        possible_actions = [\"Left\", \"Right\", \"Stop\"]\n",
    "        \n",
    "    content.append(\n",
    "            {\n",
    "                \"type\" : \"text\", \n",
    "                \"text\" : f\"\\nPossible actions: {possible_actions.__str__()}\\nNow predict the next action based on the input you have recived. Answer on the format: Action: (an the action you choose)\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages = [\n",
    "            {\"role\" : \"system\", \"content\" : [{\"type\" : \"text\", \"text\" : system_prompt}]},\n",
    "            {\"role\" : \"user\", \"content\" : content},\n",
    "        ]\n",
    "\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    images.extend([current_image])\n",
    "    \n",
    "    formatted_sample = {}\n",
    "    formatted_sample[\"text\"] = text\n",
    "    formatted_sample[\"images\"] = images\n",
    "\n",
    "    formatted_data = [formatted_sample] \n",
    "    formatted_data = DT.from_list(formatted_data)\n",
    "    return formatted_data\n",
    "    \n",
    "\n",
    "# Networking functions\n",
    "def save_image_from_base64(base64_str, filename):\n",
    "    \"\"\"Decodes the base64 string and saves the image to a file.\"\"\"\n",
    "    # Decode the base64 string to bytes\n",
    "    image_bytes = base64.b64decode(base64_str)\n",
    "    \n",
    "    # Open the image from the decoded bytes and save it\n",
    "    image = Image.open(BytesIO(image_bytes))\n",
    "    filepath = os.path.join(SAVE_DIR, filename)\n",
    "    image.save(filepath)\n",
    "\n",
    "def start_episode(path_id):\n",
    "    \"\"\"Starts a new episode and returns the session ID.\"\"\"\n",
    "    url = f\"{BASE_URL}/episode/start_episode/{path_id}\"\n",
    "    response = requests.post(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Episode started for path_id: {path_id}\")\n",
    "        data = response.json()  # The response is now in JSON format\n",
    "        save_image_from_base64(data[\"image\"], f\"{path_id}/step_{step_id}.png\")\n",
    "        \n",
    "        return path_id, data[\"distance\"], data[\"move_possible\"]\n",
    "    else:\n",
    "        print(f\"Error starting episode: {response.json()}\")\n",
    "        return None\n",
    "\n",
    "def end_episode(path_id):\n",
    "    \"\"\"Ends the current episode and returns some data\"\"\"\n",
    "    url = f\"{BASE_URL}/episode/end_episode/{path_id}\"\n",
    "    response = requests.delete(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Episode ended for path_id: {path_id}\\n\")\n",
    "        data = response.json()  # The response is now in JSON format\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Error ending episode: {response.json()}\")\n",
    "        return None\n",
    "\n",
    "def take_action(session_id, action, step_id):\n",
    "    \"\"\"Takes an action in the specified session and saves the returned image.\"\"\"\n",
    "    url = f\"{BASE_URL}/take_action/{session_id}\"\n",
    "    params = {\"action\": action}\n",
    "    response = requests.post(url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        #print(f\"Action '{action}' taken in session '{session_id}'\")\n",
    "        data = response.json()  # The response is now in JSON format\n",
    "        save_image_from_base64(data[\"image\"], f\"{path_id}/step_{step_id}.png\")\n",
    "        return data[\"distance\"], data[\"move_possible\"]\n",
    "    else:\n",
    "        print(f\"Error taking action: {response.json()}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0c184-fd99-4d9d-97db-8a0d62b53d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "set_seed(args.seed)\n",
    "# kan være at den max pixel greia var problemet og at den ikke klarte å see hele panorama bildet\n",
    "processor = AutoProcessor.from_pretrained(args.model, cache_dir=args.cache_dir)\n",
    "\n",
    "if args.version == 1:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                args.custom_model, \n",
    "                cache_dir=args.cache_dir,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                device_map=device\n",
    "            )\n",
    "for batch in data_loader:\n",
    "    batch.to(\"cuda\")\n",
    "            \n",
    "    outputs = model(**batch)\n",
    "    argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "    model_prediction = processor.decode(argmax[-1]) # is -1 because it does not predict one more\n",
    "    print(f\"Predicted action: {model_prediction}\")\n",
    "\n",
    "else:\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "                args.custom_model, \n",
    "                torch_dtype=torch.bfloat16,\n",
    "                cache_dir=args.cache_dir,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                device_map=device\n",
    "            )\n",
    "\n",
    "test_data = load_dataset(args.test_data)\n",
    "system_prompt = load_system_prompt(args.system_prompt)\n",
    "\n",
    "model.eval()\n",
    "print(\"model loaded to memory\")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12150002-5186-42bf-a272-72b70537eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = CollateFunctor(processor, args.width, args.height)\n",
    "\n",
    "model_actions = {}\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 3704 er den forrige\n",
    "    for i, path in enumerate(test_data[1310:]):\n",
    "        print(f\"Number: {i}\")\n",
    "        step_id = 0\n",
    "        path_id = path[\"path_id\"]\n",
    "        route_instruction = path[\"instructions\"][args.instruction_index]\n",
    "        previous_actions = []\n",
    "\n",
    "        # to view the predictions\n",
    "        model_predictions = []\n",
    "\n",
    "        images_path = os.path.join(SAVE_DIR, f\"{path_id}\")\n",
    "        os.makedirs(images_path, exist_ok=True)\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        model_prediction = None\n",
    "        session_id, distance, move_possible = start_episode(path_id)\n",
    "        \n",
    "        # assumes the episode is ended if candidates == None\n",
    "        while step_id < 32 and model_prediction != \"Stop\" and distance != None and move_possible != None:\n",
    "            #prompt = format_prompt_v4(path_id, step_id, instruction, previous_actions, move_possible, system_prompt, processor, data_type=\"val\")\n",
    "            prompt = format_prompt_v3_5(images_path, step_id, route_instruction, distance, previous_actions, move_possible, processor, system_prompt)\n",
    "            dataset = CustomDataset(prompt)\n",
    "            data_loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=1,\n",
    "                collate_fn=collate_fn\n",
    "            )\n",
    "    \n",
    "            for batch in data_loader:\n",
    "                batch.to(device)\n",
    "                \n",
    "                outputs = model(**batch)\n",
    "                argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "                model_prediction = processor.decode(argmax[-1]) # is -1 because it does not predict one more\n",
    "                model_predictions.append(model_prediction)\n",
    "\n",
    "                #print(f\"Model prediction: {model_prediction}\")\n",
    "                #print(f\"Previous Actions: {previous_actions.__str__()}\")\n",
    "                #print(f\"Move possible: {move_possible}\")\n",
    "                #print(processor.decode(batch[\"input_ids\"][0]))\n",
    "                \n",
    "                if model_prediction == \"Move\" and adjust == True:\n",
    "                    step_id += 1\n",
    "                    previous_actions.append(\"Automatically Turn Towards Node\")\n",
    "                    distance, move_possible = take_action(path_id, \"Adjust\", step_id)\n",
    "                \n",
    "                step_id += 1\n",
    "                previous_actions.append(model_prediction)\n",
    "                distance, move_possible = take_action(path_id, model_prediction, step_id)\n",
    "        \n",
    "        model_actions[path_id] = model_predictions\n",
    "        end_episode(path_id)\n",
    "\n",
    "with open(f\"{SAVE_DIR}-actions.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(model_actions, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4cf0-21dc-42d1-95d2-9767dc20b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{SAVE_DIR}-actions.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(model_actions, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffef8b6-dc93-4eaa-8bbf-62611eaa675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (masters-env)",
   "language": "python",
   "name": "masters-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
