{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58221c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7bc7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def calculate_results(action_space, dataset, split, base_path):\n",
    "    with open(base_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    tp = defaultdict(int)\n",
    "    fp = defaultdict(int)\n",
    "    fn = defaultdict(int)\n",
    "\n",
    "    for path_id, entry in data.items():\n",
    "        gold_seq = entry[\"gold\"]\n",
    "        pred_seq = entry[\"model\"]\n",
    "        min_len = min(len(gold_seq), len(pred_seq))\n",
    "\n",
    "        for i in range(min_len):\n",
    "            gold = gold_seq[i]\n",
    "            pred = pred_seq[i]\n",
    "            if pred == gold:\n",
    "                tp[gold] += 1\n",
    "            else:\n",
    "                fp[pred] += 1\n",
    "                fn[gold] += 1\n",
    "\n",
    "    # Get unique actions\n",
    "    actions = set(tp.keys()) | set(fp.keys()) | set(fn.keys())\n",
    "\n",
    "    # Compute per-class precision, recall, f1\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    count = 0\n",
    "\n",
    "    print(f\"Results for {split} [{action_space}]\\n\")\n",
    "    for action in sorted(actions):\n",
    "        p = tp[action] / (tp[action] + fp[action]) if (tp[action] + fp[action]) > 0 else 0\n",
    "        r = tp[action] / (tp[action] + fn[action]) if (tp[action] + fn[action]) > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "\n",
    "        print(f\"{action}: Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "        total_precision += p\n",
    "        total_recall += r\n",
    "        total_f1 += f1\n",
    "        count += 1\n",
    "\n",
    "    # Macro average\n",
    "    macro_p = total_precision / count if count > 0 else 0\n",
    "    macro_r = total_recall / count if count > 0 else 0\n",
    "    macro_f1 = total_f1 / count if count > 0 else 0\n",
    "\n",
    "    print(\"\\nMacro-Averaged Metrics\")\n",
    "    print(f\"Precision: {macro_p:.3f}\")\n",
    "    print(f\"Recall:    {macro_r:.3f}\")\n",
    "    print(f\"F1 Score:  {macro_f1:.3f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Micro-average metrics\n",
    "    total_tp = sum(tp.values())\n",
    "    total_fp = sum(fp.values())\n",
    "    total_fn = sum(fn.values())\n",
    "\n",
    "    micro_p = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    micro_r = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    micro_f1 = 2 * micro_p * micro_r / (micro_p + micro_r) if (micro_p + micro_r) > 0 else 0\n",
    "\n",
    "    print(\"\\nMicro-Averaged Metrics\")\n",
    "    print(f\"Precision: {micro_p:.3f}\")\n",
    "    print(f\"Recall:    {micro_r:.3f}\")\n",
    "    print(f\"F1 Score:  {micro_f1:.3f}\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "def print_results(action_space):\n",
    "    base_path = f\"./preprocessed-results/{action_space}\"\n",
    "    models = [\"full-data\"]\n",
    "    dataset = \"dataset_v3-full\" if action_space == \"low-level\" else \"dataset_v5-full\"\n",
    "    splits = [\"val\", \"test\"]\n",
    "\n",
    "    for model in models:\n",
    "        print(f\"model: {model}\")\n",
    "        for split in splits:\n",
    "            result_path = os.path.join(base_path, model, f\"{split}_path_dict.json\")\n",
    "            calculate_results(action_space, dataset, split, result_path)\n",
    "            \n",
    "        print(\"\\n\"*4)\n",
    "\n",
    "print_results(\"low-level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def calculate_results(action_space, dataset, split, base_path):\n",
    "    with open(base_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    tp = defaultdict(int)\n",
    "    fp = defaultdict(int)\n",
    "    fn = defaultdict(int)\n",
    "    sr = 0\n",
    "    \n",
    "    for path_id, entry in data.items():\n",
    "        gold_seq = entry[\"gold\"]\n",
    "        pred_seq = entry[\"model\"]\n",
    "        min_len = min(len(gold_seq), len(pred_seq))\n",
    "        \n",
    "        if entry[\"gold\"] == entry[\"model\"]:\n",
    "            sr += 1\n",
    "\n",
    "        for i in range(min_len):\n",
    "            gold = gold_seq[i]\n",
    "            pred = pred_seq[i]\n",
    "            if pred == gold:\n",
    "                tp[gold] += 1\n",
    "            else:\n",
    "                fp[pred] += 1\n",
    "                fn[gold] += 1\n",
    "\n",
    "    actions = set(tp.keys()) | set(fp.keys()) | set(fn.keys())\n",
    "\n",
    "    action_metrics = {}\n",
    "    total_f1 = 0\n",
    "    for action in sorted(actions):\n",
    "        p = tp[action] / (tp[action] + fp[action]) if (tp[action] + fp[action]) > 0 else 0\n",
    "        r = tp[action] / (tp[action] + fn[action]) if (tp[action] + fn[action]) > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "        action_metrics[action] = f1\n",
    "        total_f1 += f1\n",
    "\n",
    "    total_tp = sum(tp.values())\n",
    "    total = total_tp + sum(fn.values())\n",
    "    accuracy = total_tp / total if total > 0 else 0\n",
    "    macro_f1 = total_f1/len(actions)\n",
    "\n",
    "    # Example placeholder for Conservative Success Rate (you might want to replace it)\n",
    "    conservative_success_rate = sr / len(data)\n",
    "\n",
    "    return accuracy, action_metrics, conservative_success_rate, macro_f1\n",
    "\n",
    "\n",
    "def print_results(action_space):\n",
    "    base_path = f\"./preprocessed-results/{action_space}\"\n",
    "    models = [\"full-data\"]\n",
    "    dataset = \"dataset_v3-full\" if action_space == \"low-level\" else \"dataset_v5-full\"\n",
    "    splits = [\"val\", \"test\"]\n",
    "\n",
    "\n",
    "    # Prepare DataFrames to collect everything\n",
    "    results = []\n",
    "\n",
    "    for split in splits:\n",
    "        for model in models:\n",
    "            result_path = os.path.join(base_path, model, f\"{split}_path_dict.json\")\n",
    "            accuracy, action_metrics, conservative_success_rate, macro_f1 = calculate_results(action_space, dataset, split, result_path)\n",
    "            \n",
    "            row = {\n",
    "                \"Split\" : split,\n",
    "                \"Model\": model,\n",
    "                \"Accuracy\": round(accuracy, 2),\n",
    "                \"Right f1\": round(action_metrics.get(\"Right\", 0), 2),\n",
    "                \"Left f1\": round(action_metrics.get(\"Left\", 0), 2),\n",
    "                \"Move f1\": round(action_metrics.get(\"Move\", 0), 2),\n",
    "                \"Stop f1\": round(action_metrics.get(\"Stop\", 0), 2),\n",
    "                \"Macro f1\": round(macro_f1, 2),\n",
    "                \"Conservative Success Rate\": round(conservative_success_rate, 2)\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "    #val_df = pd.DataFrame(results[\"val\"])\n",
    "    #test_df = pd.DataFrame(results[\"test\"])\n",
    "\n",
    "    #print(\"Validation Split (Seen):\")\n",
    "    #print(val_df)\n",
    "    #print(\"\\nTest Split (Unseen):\")\n",
    "    #print(test_df)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "columns_order = [\n",
    "    \"Split\",\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Right f1\",\n",
    "    \"Left f1\",\n",
    "    \"Move f1\",\n",
    "    \"Stop f1\",\n",
    "    \"Macro f1\",\n",
    "    \"Conservative Success Rate\"\n",
    "]\n",
    "\n",
    "results_df = print_results(\"low-level\")\n",
    "results_df = results_df[columns_order]\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š Low-level-results\"))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b745c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(action_space):\n",
    "    base_path = f\"./preprocessed-results/{action_space}\"\n",
    "    models = [\"full-data\"]\n",
    "    dataset = \"dataset_v3-full\" if action_space == \"low-level\" else \"dataset_v5-full\"\n",
    "    splits = [\"val\", \"test\"]\n",
    "\n",
    "    # Prepare DataFrames to collect everything\n",
    "    results = []\n",
    "\n",
    "    for split in splits:\n",
    "        for model in models:\n",
    "            result_path = os.path.join(base_path, model, f\"{split}_path_dict.json\")\n",
    "            accuracy, action_metrics, conservative_success_rate, macro_f1 = calculate_results(action_space, dataset, split, result_path)\n",
    "            \n",
    "            row = {\n",
    "                \"Split\" : split,\n",
    "                \"Model\": model,\n",
    "                \"Accuracy\": round(accuracy, 2),\n",
    "                \"Stop f1\": round(action_metrics.get(\"Stop\", 0), 2),\n",
    "                \"Macro f1\": round(macro_f1, 2),\n",
    "                \"Conservative Success Rate\": round(conservative_success_rate, 2)\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "    #val_df = pd.DataFrame(results[\"val\"])\n",
    "    #test_df = pd.DataFrame(results[\"test\"])\n",
    "\n",
    "    #print(\"Validation Split (Seen):\")\n",
    "    #print(val_df)\n",
    "    #print(\"\\nTest Split (Unseen):\")\n",
    "    #print(test_df)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "columns_order = [\n",
    "    \"Split\",\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Stop f1\",\n",
    "    \"Conservative Success Rate\"\n",
    "]\n",
    "\n",
    "results_df = print_results(\"panoramic\")\n",
    "results_df = results_df[columns_order]\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š panoramic-results\"))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27230d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(action_space):\n",
    "    base_path = f\"./preprocessed-results/{action_space}\"\n",
    "    models = [\"qwen2-full\", \"qwen2.5-freeze\"]\n",
    "    dataset = \"dataset_v3_5\" if action_space == \"low-level\" else \"dataset_v5\"\n",
    "    splits = [\"val\"]\n",
    "\n",
    "    # Prepare DataFrames to collect everything\n",
    "    results = []\n",
    "\n",
    "    for split in splits:\n",
    "        for model in models:\n",
    "            result_path = os.path.join(base_path, model, \"zero-shot.json\")\n",
    "            accuracy, action_metrics, conservative_success_rate, macro_f1 = calculate_results(action_space, dataset, split, result_path)\n",
    "            print(macro_f1)\n",
    "            row = {\n",
    "                \"Split\" : split,\n",
    "                \"Model\": model,\n",
    "                \"Accuracy\": round(accuracy, 2),\n",
    "                \"Right f1\": round(action_metrics.get(\"Right\", 0), 2),\n",
    "                \"Left f1\": round(action_metrics.get(\"Left\", 0), 2),\n",
    "                \"Move f1\": round(action_metrics.get(\"Move\", 0), 2),\n",
    "                \"Stop f1\": round(action_metrics.get(\"Stop\", 0), 2),\n",
    "                \"Macro f1\": round(macro_f1, 2),\n",
    "                \"Conservative Success Rate\": round(conservative_success_rate, 2)\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "    #val_df = pd.DataFrame(results[\"val\"])\n",
    "    #test_df = pd.DataFrame(results[\"test\"])\n",
    "\n",
    "    #print(\"Validation Split (Seen):\")\n",
    "    #print(val_df)\n",
    "    #print(\"\\nTest Split (Unseen):\")\n",
    "    #print(test_df)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "columns_order = [\n",
    "    \"Split\",\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Right f1\",\n",
    "    \"Left f1\",\n",
    "    \"Move f1\",\n",
    "    \"Stop f1\",\n",
    "    \"Macro f1\",\n",
    "    \"Conservative Success Rate\"\n",
    "]\n",
    "\n",
    "results_df = print_results(\"low-level\")\n",
    "results_df = results_df[columns_order]\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š Low-level-results\"))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f067ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(action_space):\n",
    "    base_path = f\"./preprocessed-results/{action_space}\"\n",
    "    models = [\"qwen2-full\", \"qwen2.5-freeze\"]\n",
    "    dataset = \"dataset_v3_5\" if action_space == \"low-level\" else \"dataset_v5\"\n",
    "    splits = [\"val\"]\n",
    "\n",
    "    # Prepare DataFrames to collect everything\n",
    "    results = []\n",
    "\n",
    "    for split in splits:\n",
    "        for model in models:\n",
    "            result_path = os.path.join(base_path, model, \"zero-shot.json\")\n",
    "            accuracy, action_metrics, conservative_success_rate, macro_f1 = calculate_results(action_space, dataset, split, result_path)\n",
    "            print(macro_f1)\n",
    "            row = {\n",
    "                \"Split\" : split,\n",
    "                \"Model\": model,\n",
    "                \"Accuracy\": round(accuracy, 2),\n",
    "                \"Stop f1\": round(action_metrics.get(\"Stop\", 0), 2),\n",
    "                \"Macro f1\": round(macro_f1, 2),\n",
    "                \"Conservative Success Rate\": round(conservative_success_rate, 2)\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "    #val_df = pd.DataFrame(results[\"val\"])\n",
    "    #test_df = pd.DataFrame(results[\"test\"])\n",
    "\n",
    "    #print(\"Validation Split (Seen):\")\n",
    "    #print(val_df)\n",
    "    #print(\"\\nTest Split (Unseen):\")\n",
    "    #print(test_df)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "columns_order = [\n",
    "    \"Split\",\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Stop f1\",\n",
    "    \"Macro f1\",\n",
    "    \"Conservative Success Rate\"\n",
    "]\n",
    "\n",
    "results_df = print_results(\"panoramic\")\n",
    "results_df = results_df[columns_order]\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š Panoramic -zero-shot-results\"))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7afb3f",
   "metadata": {},
   "source": [
    "# No-adjust action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b843ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(action_space):\n",
    "    base_path = f\"./preprocessed-results/{action_space}\"\n",
    "    models = [\"qwen2.5-freezed\"]\n",
    "    dataset = \"dataset_v3_5\" if action_space == \"low-level\" else \"dataset_v5\"\n",
    "    splits = [\"val\", \"test\"]\n",
    "\n",
    "    # Prepare DataFrames to collect everything\n",
    "    results = []\n",
    "\n",
    "    for split in splits:\n",
    "        for model in models:\n",
    "            result_path = os.path.join(base_path, model, f\"{split}_path_dict.json\")\n",
    "            accuracy, action_metrics, conservative_success_rate, macro_f1 = calculate_results(action_space, dataset, split, result_path)\n",
    "            print(macro_f1)\n",
    "            row = {\n",
    "                \"Split\" : split,\n",
    "                \"Model\": model,\n",
    "                \"Accuracy\": round(accuracy, 2),\n",
    "                \"Right f1\": round(action_metrics.get(\"Right\", 0), 2),\n",
    "                \"Left f1\": round(action_metrics.get(\"Left\", 0), 2),\n",
    "                \"Move f1\": round(action_metrics.get(\"Move\", 0), 2),\n",
    "                \"Stop f1\": round(action_metrics.get(\"Stop\", 0), 2),\n",
    "                \"Macro f1\": round(macro_f1, 2),\n",
    "                \"Conservative Success Rate\": round(conservative_success_rate, 2)\n",
    "            }\n",
    "            results.append(row)\n",
    "\n",
    "    #val_df = pd.DataFrame(results[\"val\"])\n",
    "    #test_df = pd.DataFrame(results[\"test\"])\n",
    "\n",
    "    #print(\"Validation Split (Seen):\")\n",
    "    #print(val_df)\n",
    "    #print(\"\\nTest Split (Unseen):\")\n",
    "    #print(test_df)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# Run the function\n",
    "columns_order = [\n",
    "    \"Split\",\n",
    "    \"Model\",\n",
    "    \"Accuracy\",\n",
    "    \"Right f1\",\n",
    "    \"Left f1\",\n",
    "    \"Move f1\",\n",
    "    \"Stop f1\",\n",
    "    \"Macro f1\",\n",
    "    \"Conservative Success Rate\"\n",
    "]\n",
    "\n",
    "results_df = print_results(\"82-vfov\")\n",
    "results_df = results_df[columns_order]\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š Low-level-results\"))\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plotting style\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "\n",
    "# Data (manually entered from your table)\n",
    "data = [\n",
    "    # Val Seen\n",
    "    [\"Val Seen\", \"Qwen2-VL-full\", \"Right\", 0.65],\n",
    "    [\"Val Seen\", \"Qwen2-VL-full\", \"Left\", 0.59],\n",
    "    [\"Val Seen\", \"Qwen2-VL-full\", \"Move\", 0.57],\n",
    "    [\"Val Seen\", \"Qwen2-VL-full\", \"Stop\", 0.71],\n",
    "    [\"Val Seen\", \"Qwen2-VL-frozen\", \"Right\", 0.70],\n",
    "    [\"Val Seen\", \"Qwen2-VL-frozen\", \"Left\", 0.68],\n",
    "    [\"Val Seen\", \"Qwen2-VL-frozen\", \"Move\", 0.73],\n",
    "    [\"Val Seen\", \"Qwen2-VL-frozen\", \"Stop\", 0.78],\n",
    "    [\"Val Seen\", \"Qwen2.5-VL-frozen\", \"Right\", 0.71],\n",
    "    [\"Val Seen\", \"Qwen2.5-VL-frozen\", \"Left\", 0.69],\n",
    "    [\"Val Seen\", \"Qwen2.5-VL-frozen\", \"Move\", 0.74],\n",
    "    [\"Val Seen\", \"Qwen2.5-VL-frozen\", \"Stop\", 0.78],\n",
    "    \n",
    "    # Val Unseen\n",
    "    [\"Val Unseen\", \"Qwen2-VL-full\", \"Right\", 0.66],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-full\", \"Left\", 0.63],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-full\", \"Move\", 0.58],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-full\", \"Stop\", 0.72],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-frozen\", \"Right\", 0.66],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-frozen\", \"Left\", 0.68],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-frozen\", \"Move\", 0.70],\n",
    "    [\"Val Unseen\", \"Qwen2-VL-frozen\", \"Stop\", 0.74],\n",
    "    [\"Val Unseen\", \"Qwen2.5-VL-frozen\", \"Right\", 0.70],\n",
    "    [\"Val Unseen\", \"Qwen2.5-VL-frozen\", \"Left\", 0.72],\n",
    "    [\"Val Unseen\", \"Qwen2.5-VL-frozen\", \"Move\", 0.72],\n",
    "    [\"Val Unseen\", \"Qwen2.5-VL-frozen\", \"Stop\", 0.74],\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Split\", \"Model\", \"Action\", \"F1 Score\"])\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "# Create one plot per split\n",
    "for split in [\"Val Seen\", \"Val Unseen\"]:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.barplot(data=df[df[\"Split\"] == split], x=\"Action\", y=\"F1 Score\", hue=\"Model\", palette=palette)\n",
    "    plt.title(f\"F1 Scores by Action - {split}\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(title=\"Model\")\n",
    "\n",
    "    # Add value labels manually\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{height:.2f}',\n",
    "                        (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom',\n",
    "                        fontsize=9, xytext=(0, 3), textcoords='offset points')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plotting style\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "mpl.rcParams['ps.fonttype'] = 42\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# Load both splits\n",
    "with open(\"../tasks/R2R/data/R2R_val_seen.json\", \"r\", encoding=\"utf-8\") as f_seen:\n",
    "    data_seen = json.load(f_seen)\n",
    "\n",
    "with open(\"../tasks/R2R/data/R2R_val_unseen.json\", \"r\", encoding=\"utf-8\") as f_unseen:\n",
    "    data_unseen = json.load(f_unseen)\n",
    "\n",
    "def avg_lengths(data):\n",
    "    totals = [0, 0, 0]\n",
    "    for sample in data:\n",
    "        for i in range(3):\n",
    "            totals[i] += len(sample[\"instructions\"][i].split(\" \"))\n",
    "    return [total / len(data) for total in totals]\n",
    "\n",
    "seen_avg = avg_lengths(data_seen)\n",
    "unseen_avg = avg_lengths(data_unseen)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Split\": [\"Val Seen\"] * 3 + [\"Val Unseen\"] * 3,\n",
    "    \"Instruction Index\": [0, 1, 2] * 2,\n",
    "    \"Avg Length\": seen_avg + unseen_avg\n",
    "})\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "palette = sns.color_palette(\"colorblind\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(data=df, x=\"Instruction Index\", y=\"Avg Length\", hue=\"Split\", palette=palette)\n",
    "\n",
    "plt.ylim(0, max(df[\"Avg Length\"]) + 11)\n",
    "plt.xlabel(\"Instruction Index\", fontsize=24)\n",
    "plt.ylabel(\"Average Length (words)\", fontsize=24)\n",
    "plt.legend(title=\"Data Split\", fontsize=16, title_fontsize=16)\n",
    "\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=18)\n",
    "# Add value labels\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.annotate(f'{height:.1f}',\n",
    "                    (p.get_x() + p.get_width() / 2., height),\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=18, xytext=(0, 3), textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"../figures/plots/instruction-lenths-over-indexes.pdf\", bbox_inches=\"tight\", dpi=700)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
