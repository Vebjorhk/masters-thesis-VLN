{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d73774-7f83-41ae-94ae-75213b9818f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from PIL import Image\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "#from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration,  AutoProcessor\n",
    "from datasets import Dataset as DF\n",
    "\n",
    "\n",
    "from utils import set_seed, load_dataset, get_device, load_system_prompt, grouped_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656646f-d851-44d5-a3ea-877811152755",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "def create_model(model_id, cache_dir, use_flash_attention):\n",
    "    if use_flash_attention:\n",
    "        # husk å endre tilbake\n",
    "        return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            model_id, \n",
    "            cache_dir = cache_dir,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            device_map=device,\n",
    "        )\n",
    "    \n",
    "    return Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_id, \n",
    "        cache_dir = cache_dir,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=device\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.text = data[\"text\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "        self.panoramas = data[\"panoramas\"]\n",
    "        self.candidates = data[\"candidates\"]\n",
    "        self.path_id = data[\"path_id\"]\n",
    "        self.step_id = data[\"step_id\"]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.text[index], self.panoramas[index], self.candidates[index], self.labels[index], self.path_id[index], self.step_id[index]\n",
    "\n",
    "\n",
    "# TODO: make the collatefunctor work with batches\n",
    "class CollateFunctor:\n",
    "    # No batch, therefore no max length\n",
    "    def __init__(self, processor, width, height):\n",
    "        self.processor = processor\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        text, panoramas, candidates, gold_candidate, path_id, step_id = batch[0]\n",
    "        candidate_tokens = self.processor.tokenizer(f\"{gold_candidate}\", return_tensors=\"pt\").input_ids\n",
    "        # Explicitly add the Candidate: \n",
    "        label_start = self.processor.tokenizer(\"<|im_start|>assistant\\nCandidate: \", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        # 1440 x 360\n",
    "        images = [Image.open(img).resize((960, 240), Image.Resampling.LANCZOS) for img in panoramas]\n",
    "        candidate_images = [Image.open(img).resize((self.width, self.height), Image.Resampling.LANCZOS) for img in candidates]\n",
    "        \n",
    "        images.extend(candidate_images)\n",
    "        \n",
    "        processed = self.processor(text=text, images=[images], return_tensors=\"pt\")\n",
    "\n",
    "        prompt_input_ids = processed[\"input_ids\"]\n",
    "        prompt_input_ids = torch.cat([prompt_input_ids, label_start], dim=1)\n",
    "        input_ids = torch.cat([prompt_input_ids, candidate_tokens], dim=1)\n",
    "        \n",
    "        labels = torch.cat(\n",
    "            [\n",
    "                # se på denne senere, altså om vi skal virkelig fokusere på alt det der andre chat greiene\n",
    "                torch.tensor([-100]*len(prompt_input_ids[0])).unsqueeze(0),\n",
    "                candidate_tokens\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        attention_mask = torch.ones(1, input_ids.shape[1])\n",
    "        processed[\"labels\"] = labels\n",
    "        processed[\"input_ids\"] = input_ids\n",
    "        processed[\"attention_mask\"] = attention_mask\n",
    "        processed[\"gold_candidate\"] = candidate_tokens\n",
    "        processed[\"path_id\"] = path_id\n",
    "        processed[\"step_id\"] = step_id\n",
    "        \n",
    "        return processed\n",
    "\n",
    "\n",
    "def format_prompts_v5(dataset, processor, system_prompt, instruction_index, path, data_type=\"train\"):\n",
    "    root_path = os.path.join(path, data_type)\n",
    "    formatted_data = []\n",
    "\n",
    "    current_path_id = None\n",
    "    distance_traveled = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "        path_id = sample[\"path_id\"]\n",
    "        step_id = sample[\"step_id\"]\n",
    "        route_instruction = sample[\"instructions\"][instruction_index]\n",
    "\n",
    "        # should be in the order: panorama_history, current_panorama, candidates views from left to right\n",
    "        current_panorama = os.path.join(root_path, sample[\"current_image\"])\n",
    "        panoramas = [os.path.join(root_path, i) for i in sample[\"image_history\"]]\n",
    "        \n",
    "        if current_path_id != path_id:\n",
    "            distance_traveled = 0\n",
    "            current_path_id = path_id\n",
    "\n",
    "        # route instruction, current step, cumulative distance\n",
    "        content = [\n",
    "            {\n",
    "                \"type\" : \"text\",\n",
    "                \"text\" : f\"Route instruction: {route_instruction}\\nCurrent step: {step_id}\\nCumulative Distance Traveled: {distance_traveled} meters\\n\\nPanorama Images from Previous Steps:\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # panorama from previous steps\n",
    "        for i, img in enumerate(panoramas):\n",
    "            content.append({\n",
    "                \"type\" : \"text\",\n",
    "                \"text\" : f\"\\n\\tPanorama at step: {i}: \"\n",
    "            })\n",
    "            content.append({\n",
    "                \"type\" : \"image\",\n",
    "                \"image\" : img\n",
    "            })\n",
    "\n",
    "        if len(panoramas) == 0:\n",
    "            content[0][\"text\"] += f\"[]\"\n",
    "\n",
    "        # current panorama\n",
    "        content.append({\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : f\"\\n\\nCurrent Panorama Image:\\n\\t\"\n",
    "        })\n",
    "\n",
    "        content.append({\n",
    "            \"type\" : \"image\",\n",
    "            \"image\" : current_panorama\n",
    "        })\n",
    "\n",
    "        # candidate directions\n",
    "        content.append({\n",
    "            \"type\" : \"text\",\n",
    "            \"text\" : \"\\n\\nCandidate Directions:\"\n",
    "        })\n",
    "\n",
    "        candidates = []\n",
    "        for i, candidate in enumerate(sample[\"candidates\"].values()):\n",
    "            relative_angle = round(candidate[\"relative_angle\"], 0)\n",
    "            distance = round(candidate[\"distance\"], 2)\n",
    "            direction = \"Left\" if relative_angle < 0 else \"Right\"\n",
    "            candidate_image = os.path.join(root_path, candidate[\"image_path\"])\n",
    "            \n",
    "            content.append({\n",
    "                \"type\" : \"text\",\n",
    "                \"text\" : f\"\\n\\tCandidate: {i}:\\n\\t\\tRelative angle: {abs(relative_angle)} degrees to the {direction}\\n\\t\\tDistance: {distance} meters\\n\\t\\tview: \"\n",
    "            })\n",
    "\n",
    "            content.append({\n",
    "                \"type\" : \"image\",\n",
    "                \"image\" : candidate_image\n",
    "            })\n",
    "\n",
    "            candidates.append(candidate_image)\n",
    "\n",
    "        # adds candidate STOP and the select cnadidate view \n",
    "        # hvis du skal trene flere modeller så vær oppmerksom på dette med at du bruker stor STOP i prompten men fasit er egt liten Stop action, veldig dum feil\n",
    "        content.append({\n",
    "            \"type\" : \"text\", # remember the Stop thing when writing later, this hadnt any impact on initial accuracy\n",
    "            \"text\" : \"\\n\\tCandidate: Stop\\n\\nNow, analyze the route instruction, your current position, and the available candidate directions. Select the candidate that best matches the instruction and helps you continue along the correct path. Answer on the format: Candidate: (and then the number)\"\n",
    "        })\n",
    "        # HUSK AT DU HAR ENDRET STOP til Stop\n",
    "        messages = [\n",
    "            {\"role\" : \"system\", \"content\" : [{\"type\" : \"text\", \"text\" : system_prompt}]},\n",
    "            {\"role\" : \"user\", \"content\" : content},\n",
    "        ]\n",
    "\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "        panoramas.extend([current_panorama])\n",
    "\n",
    "        formatted_sample = {}\n",
    "        formatted_sample[\"text\"] = text\n",
    "        formatted_sample[\"labels\"] = str(sample[\"gold_label\"])\n",
    "        formatted_sample[\"candidates\"] = candidates\n",
    "        formatted_sample[\"panoramas\"] = panoramas\n",
    "        formatted_sample[\"path_id\"] = path_id\n",
    "        formatted_sample[\"step_id\"] = step_id\n",
    "\n",
    "        formatted_data.append(formatted_sample)\n",
    "        \n",
    "        # update variables:\n",
    "        if sample[\"gold_label\"] != \"Stop\":\n",
    "            distance_traveled = round(sample[\"candidates\"][str(sample[\"gold_label\"])][\"distance\"] + distance_traveled, 2)\n",
    "\n",
    "    formatted_data = DF.from_list(formatted_data)\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "def run_epoch_train(model, processor, dataloader, optimizer, lr_scheduler, epoch, batch_size, grouped_data):\n",
    "    print(f\"Training model, epoch: {epoch}\")\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    batch_loss = 0\n",
    "    \n",
    "    model_loss_analyse = []\n",
    "\n",
    "    episode_counter = 0\n",
    "    batch_counter = 0\n",
    "\n",
    "    l = 0\n",
    "    # this list contains the number of samples which to be devided with by when accumulating batches\n",
    "    batch_length_list = []\n",
    "    for i, (k, v) in enumerate(grouped_data.items()):\n",
    "        l += len(v)\n",
    "        \n",
    "        if (i % batch_size == 0 and i != 0) or i+1 == len(grouped_data):\n",
    "            batch_length_list.append(l)\n",
    "            l = 0\n",
    "\n",
    "    batch_length = batch_length_list[batch_counter]\n",
    "    print(batch_length)\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        gold_candidate = batch[\"gold_candidate\"]\n",
    "        path_id = batch[\"path_id\"]\n",
    "        step_id = batch[\"step_id\"]\n",
    "        \n",
    "        del batch[\"gold_candidate\"]\n",
    "        del batch[\"path_id\"]\n",
    "        del batch[\"step_id\"]\n",
    "\n",
    "        batch.to(device)\n",
    "    \n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        batch_loss += loss.item()\n",
    "        epoch_loss += loss.item()  \n",
    "        \n",
    "        (loss/batch_length).backward()\n",
    "\n",
    "        wandb.log({\"step_loss\" : loss.item()})\n",
    "\n",
    "        # get argmax and -2 because it will predict one more during traning\n",
    "        argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "        model_prediction = processor.decode(argmax[-2])\n",
    "        gold = processor.decode(gold_candidate[0])\n",
    "        \n",
    "        sample = {}\n",
    "        sample[\"loss\"] = loss.item() # dette var problemet\n",
    "        sample[\"path_id\"] = path_id\n",
    "        sample[\"step_id\"] = step_id\n",
    "        sample[\"gold_label\"] = gold\n",
    "        sample[\"argmax_result\"] = model_prediction\n",
    "\n",
    "        model_loss_analyse.append(sample)\n",
    "\n",
    "        if (episode_counter % batch_size == 0 and episode_counter != 0 and gold == \"Stop\") or (i + 1) == len(dataloader):\n",
    "            wandb.log({\"episode_loss\" : batch_loss/batch_length})\n",
    "            \n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step() # never remove this again, this is by the book and should alawys be like this\n",
    "            \n",
    "            optimizer.zero_grad()  # Reset gradients for next accumulation\n",
    "            \n",
    "            batch_loss = 0\n",
    "            batch_counter += 1\n",
    "            if batch_counter < len(batch_length_list):\n",
    "                batch_length = batch_length_list[batch_counter]\n",
    "\n",
    "        if gold == \"Stop\":\n",
    "            episode_counter += 1\n",
    "\n",
    "\n",
    "    epoch_loss /= len(dataloader)\n",
    "    wandb.log({\"training_loss\": epoch_loss, \"learning_rate\": lr_scheduler.get_last_lr()[0]})\n",
    "    \n",
    "    return epoch_loss, model_loss_analyse\n",
    "    \n",
    "\n",
    "@torch.no_grad\n",
    "def run_epoch_eval_or_test(model, processor, dataloader, mode=\"val\"):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    accuracy = 0\n",
    "    stop_total = 0\n",
    "    stop = 0\n",
    "    stop_pred_total = 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch.to(device)\n",
    "            gold_candidate = batch[\"gold_candidate\"]\n",
    "            path_id = batch[\"path_id\"]\n",
    "            step_id = batch[\"step_id\"]\n",
    "            \n",
    "            del batch[\"gold_candidate\"]\n",
    "            del batch[\"path_id\"]\n",
    "            del batch[\"step_id\"]\n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            total_loss += outputs.loss.item()\n",
    "\n",
    "            # greedy search for action\n",
    "            argmax = torch.argmax(outputs.logits, dim=2)[0]\n",
    "\n",
    "            model_prediction = processor.decode(argmax[-2])\n",
    "            gold = processor.decode(gold_candidate[0])\n",
    "\n",
    "            predictions.append({\"gold\" : gold, \"model\" : model_prediction, \"path_id\" : path_id, \"step_id\" : step_id})\n",
    "            \n",
    "            if model_prediction == gold:\n",
    "                accuracy += 1\n",
    "\n",
    "            if gold == \"Stop\":\n",
    "                stop_total += 1\n",
    "                if model_prediction == \"Stop\":\n",
    "                    stop += 1\n",
    "\n",
    "            if model_prediction == \"Stop\":\n",
    "                stop_pred_total += 1\n",
    "\n",
    "        sorted_predictions = sorted(predictions, key=lambda x: (x[\"path_id\"], x[\"step_id\"]), reverse=False)\n",
    "        path_dict = {}\n",
    "    \n",
    "        for d in sorted_predictions:\n",
    "            if path_dict.get(d[\"path_id\"], None) != None:\n",
    "                path_dict[d[\"path_id\"]][\"gold\"].append(d[\"gold\"])\n",
    "                path_dict[d[\"path_id\"]][\"model\"].append(d[\"model\"])\n",
    "        \n",
    "            else:\n",
    "                path_dict[d[\"path_id\"]] = {}\n",
    "                path_dict[d[\"path_id\"]][\"gold\"] = [d[\"gold\"]]\n",
    "                path_dict[d[\"path_id\"]][\"model\"] = [d[\"model\"]]\n",
    "\n",
    "        sr = 0\n",
    "        for path, d in path_dict.items():\n",
    "            if d[\"gold\"] == d[\"model\"]:\n",
    "                sr+=1\n",
    "                \n",
    "        sucess_rate = sr/len(path_dict)\n",
    "        total_loss /= len(dataloader)\n",
    "        accuracy = accuracy / len(dataloader)\n",
    "        stop_precision = stop / stop_pred_total if stop_pred_total > 0 else 0.0\n",
    "        \n",
    "        \n",
    "        if mode == \"val\":\n",
    "             wandb.log({\n",
    "                \"validation_loss\" : total_loss, \n",
    "                \"validation accuracy\" : accuracy, \n",
    "                \"validation sucess rate\" : sucess_rate,\n",
    "                \"validation: Stop recall\" : stop/stop_total if stop_total > 0 else 0.0,\n",
    "                \"validation Stop Precision\" : stop_precision\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            wandb.log({\n",
    "                \"test_loss\" : total_loss, \n",
    "                \"test accuracy\" : accuracy, \n",
    "                \"test sucess rate\" : sucess_rate,\n",
    "                \"test: Stop recall\" : stop/stop_total if stop_total > 0 else 0.0,\n",
    "                \"test Stop Precision\" : stop_precision\n",
    "                \n",
    "            })\n",
    "\n",
    "        # infere these model actions from the test set as it is not shuffled\n",
    "        return total_loss, path_dict\n",
    "    \n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "        self.custom_model = \"\"\n",
    "        self.cache_dir = \"\"\n",
    "\n",
    "        # these should all have the same name\n",
    "        self.wandb_name = \"Qwen2_5-panoramic-r2r-full\"\n",
    "        self.outputs_path = \"./experiment-outputs/Qwen2_5-panoramic-r2r-full\"\n",
    "        self.model_checkpoint_path =\"/\"\n",
    "\n",
    "        # for dataset\n",
    "        self.train_data = \"/dataset/train/train_data.json\"\n",
    "        self.val_data = \"/dataset/val/val_data.json\"\n",
    "        self.test_data = \"/dataset/test/test_data.json\"\n",
    "        self.dataset_root = \"/datase/\"\n",
    "        self.system_prompt = \"./prompts/system_prompt_panoramic.json\"\n",
    "\n",
    "        # hyperparameters\n",
    "        self.lr = 0.00001\n",
    "        self.weight_decay = 0.1\n",
    "        self.warmup_ratio = 0.1\n",
    "        self.shuffle = False # MUST NEVER BE SHUFFLED IN THIS NOTEBOOK\n",
    "        self.epochs = 3 # go through each route instruction once\n",
    "        \n",
    "        self.notes = \"Finetune of Qwen2.5 panoramic with freezed vision on the full R2R dataset\"\n",
    "\n",
    "        self.batch_size = 1\n",
    "        self.instruction_index_train = 0\n",
    "        self.instruction_index_val = 2\n",
    "        self.freeze_vision = True\n",
    "        self.use_flash_attention = True\n",
    "        self.width = 320\n",
    "        self.height = 240\n",
    "        self.seed = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa24e8-f1fe-45e8-bf7f-4fc06f8b2d9d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = Args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    # for saving outputs\n",
    "    if not os.path.exists(args.outputs_path):\n",
    "        os.makedirs(args.outputs_path)\n",
    "        \n",
    "    model = create_model(args.model, args.cache_dir, args.use_flash_attention)\n",
    "    processor = AutoProcessor.from_pretrained(args.model, cache_dir=args.cache_dir)\n",
    "\n",
    "    if args.freeze_vision:\n",
    "        for name, param in model.visual.named_parameters():\n",
    "            param.requires_grad = False  # Freeze parameter\n",
    "\n",
    " \n",
    "    train_data = load_dataset(args.train_data)\n",
    "    grouped_data = grouped_paths(train_data)\n",
    "\n",
    "    val_data = load_dataset(args.val_data)\n",
    "    test_data = load_dataset(args.test_data)\n",
    "    system_prompt = load_system_prompt(args.system_prompt)\n",
    "\n",
    "    val = format_prompts_v5(val_data, processor, system_prompt, args.instruction_index_val, args.dataset_root, data_type=\"val\")\n",
    "    val_dataset = CustomDataset(val)\n",
    "    collate_functor = CollateFunctor(processor, args.width, args.height)\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1, # Enforce batch size of 1 as phi-3.5 vision does not support a larger batch size out of the box (qwen does but i'll look at that later)\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_functor,\n",
    "    )\n",
    "\n",
    "    # len(grouped_data) = how many times the optimizer.step is called\n",
    "    total_steps = (len(grouped_data)/args.batch_size)*args.epochs\n",
    "    warmup_steps = int(args.warmup_ratio * total_steps)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"masters-thesis\",\n",
    "        name=args.wandb_name,\n",
    "        notes=args.notes,\n",
    "        # notes=\"This is the fine tuning of Qwen2-VL-2B-Instruct on the room-to-room dataset\",\n",
    "        job_type=\"fine-tune\",\n",
    "    \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"model\" :  args.model,\n",
    "            \"dataset\": \"room-to-room\",\n",
    "            \"epochs\": args.epochs,\n",
    "            \"batch_size\" : args.batch_size,\n",
    "            \"learning_rate\": args.lr,\n",
    "            \"weight_decay\" : args.weight_decay,\n",
    "            \"warmup_ratio\" : args.warmup_ratio,\n",
    "            \"optimizer\" : \"AdamW\",\n",
    "            \"schedular\" : \"linear_schedule_with_warmup\",\n",
    "            \"architecture\": \"transformer\",\n",
    "            \"seed\" : args.seed,\n",
    "            \"action_space\" : \"panoramic\",\n",
    "            \"model_checkpoint_path\" : args.model_checkpoint_path,\n",
    "            \"train_data_path\" : args.train_data,\n",
    "            \"prompt_path\" : args.system_prompt,\n",
    "            \"outputs_path\" : args.outputs_path,\n",
    "            \"samples\" : len(train_data),\n",
    "            \"instruction_index_train\" : \"all three instructions\", #args.instruction_index_train,\n",
    "            \"instruction_index_val\" : args.instruction_index_val\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(args.epochs):\n",
    "        instruction = i\n",
    "        \n",
    "        if i > 2:\n",
    "            instruction = i - 3\n",
    "\n",
    "        print(f\"instruction number: {instruction}\")\n",
    "        formatted_train = format_prompts_v5(train_data, processor, system_prompt, instruction, args.dataset_root, data_type=\"train\")\n",
    "        \n",
    "        dataset = CustomDataset(formatted_train)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=1, \n",
    "            shuffle=False,\n",
    "            collate_fn=collate_functor\n",
    "        )\n",
    "\n",
    "        # calculating loss on validation data\n",
    "        val_loss, path_dict = run_epoch_eval_or_test(model, processor, val_dataloader, mode=\"val\")\n",
    "\n",
    "        with open(os.path.join(args.outputs_path, f\"val_path_dict_epoch_{i}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(path_dict, file, indent=4) \n",
    "\n",
    "        # training model\n",
    "        train_loss, model_analyse = run_epoch_train(\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            dataloader=dataloader,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler=scheduler,\n",
    "            epoch=i,\n",
    "            batch_size=args.batch_size,\n",
    "            grouped_data=grouped_data\n",
    "        )\n",
    "\n",
    "        #val_loss, path_dict = run_epoch_eval_or_test(model, processor, val_dataloader, mode=\"val\")\n",
    "        \n",
    "        with open(os.path.join(args.outputs_path, f\"mode_analyse_{i}.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(model_analyse, file, indent=4)\n",
    "\n",
    "        model_save_path = os.path.join(args.model_checkpoint_path, f\"checkpoint_{i}\")\n",
    "        model.save_pretrained(model_save_path)\n",
    "        print(f\"model saved at: {model_save_path}, epoch {i} finished\")\n",
    "\n",
    "    val_loss, val_path_dict = run_epoch_eval_or_test(model, processor, val_dataloader, mode=\"val\")\n",
    "    end = time.time()\n",
    "    print(f\"training time: {(end-start)/60} minutes\")\n",
    "\n",
    "    test = format_prompts_v5(test_data, processor, system_prompt, args.instruction_index_val, args.dataset_root, data_type=\"test\")\n",
    "    test_dataset = CustomDataset(test)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1, # Enforce batch size of 1 as phi-3.5 vision does not support a larger batch size out of the box (qwen does but i'll look at that later)\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_functor,\n",
    "    )\n",
    "\n",
    "    test_loss, test_path_dict = run_epoch_eval_or_test(model, processor, test_dataloader, mode=\"test\")\n",
    "\n",
    "    with open(os.path.join(args.outputs_path, \"val_path_dict.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(val_path_dict, file, indent=4)\n",
    "\n",
    "    with open(os.path.join(args.outputs_path,\"test_path_dict.json\"), \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(test_path_dict, file, indent=4)\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c42916-707e-4747-b417-6d37c54b1bbf",
   "metadata": {},
   "source": [
    "Over er trenings koden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in5550",
   "language": "python",
   "name": "in5550"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
